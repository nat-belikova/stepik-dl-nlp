{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "latex_envs": {
      "LaTeX_envs_menu_present": true,
      "autoclose": false,
      "autocomplete": true,
      "bibliofile": "biblio.bib",
      "cite_by": "apalike",
      "current_citInitial": 1,
      "eqLabelWithNumbers": true,
      "eqNumInitial": 1,
      "hotkeys": {
        "equation": "Ctrl-E",
        "itemize": "Ctrl-I"
      },
      "labels_anchors": false,
      "latex_user_defs": false,
      "report_style_numbering": false,
      "user_envs_cfg": false
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": true
    },
    "colab": {
      "name": "task1_20newsgroups.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EaMbMon-Wah5",
        "colab_type": "text"
      },
      "source": [
        "# Тематическая классификация длинных текстов - TFIDF и LogReg"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3DWJMigdWah8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4568aa7b-1ab9-42d7-fa2f-fbcfd6ddc499"
      },
      "source": [
        "# Если Вы запускаете ноутбук на colab или kaggle,\n",
        "# выполните следующие строчки, чтобы подгрузить библиотеку dlnlputils:\n",
        " \n",
        "!git clone https://github.com/Samsung-IT-Academy/stepik-dl-nlp.git && pip install -r stepik-dl-nlp/requirements.txt\n",
        "import sys; sys.path.append('./stepik-dl-nlp')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'stepik-dl-nlp'...\n",
            "remote: Enumerating objects: 7, done.\u001b[K\n",
            "remote: Counting objects:  14% (1/7)\u001b[K\rremote: Counting objects:  28% (2/7)\u001b[K\rremote: Counting objects:  42% (3/7)\u001b[K\rremote: Counting objects:  57% (4/7)\u001b[K\rremote: Counting objects:  71% (5/7)\u001b[K\rremote: Counting objects:  85% (6/7)\u001b[K\rremote: Counting objects: 100% (7/7)\u001b[K\rremote: Counting objects: 100% (7/7), done.\u001b[K\n",
            "remote: Compressing objects: 100% (5/5), done.\u001b[K\n",
            "remote: Total 273 (delta 3), reused 5 (delta 2), pack-reused 266\u001b[K\n",
            "Receiving objects: 100% (273/273), 42.13 MiB | 20.71 MiB/s, done.\n",
            "Resolving deltas: 100% (132/132), done.\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from -r stepik-dl-nlp/requirements.txt (line 1)) (0.22.2.post1)\n",
            "Collecting spacy-udpipe\n",
            "  Downloading https://files.pythonhosted.org/packages/81/ff/878cb73163141ecb34e19b0008cb064cceb4ce6c1070d04d180c6a5d1d10/spacy_udpipe-0.3.1-py3-none-any.whl\n",
            "Collecting pymorphy2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/33/fff9675c68b5f6c63ec8c6e6ff57827dda28a1fa5b2c2d727dffff92dd47/pymorphy2-0.8-py2.py3-none-any.whl (46kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 3.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.2 in /usr/local/lib/python3.6/dist-packages (from -r stepik-dl-nlp/requirements.txt (line 4)) (1.5.1+cu101)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from -r stepik-dl-nlp/requirements.txt (line 5)) (3.2.2)\n",
            "Collecting ipymarkup\n",
            "  Downloading https://files.pythonhosted.org/packages/bf/9b/bf54c98d50735a4a7c84c71e92c5361730c878ebfe903d2c2d196ef66055/ipymarkup-0.9.0-py3-none-any.whl\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.6/dist-packages (from -r stepik-dl-nlp/requirements.txt (line 7)) (4.2.6)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from -r stepik-dl-nlp/requirements.txt (line 8)) (1.4.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from -r stepik-dl-nlp/requirements.txt (line 9)) (1.0.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from -r stepik-dl-nlp/requirements.txt (line 10)) (4.41.1)\n",
            "Collecting youtokentome\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/65/4a86cf99da3f680497ae132329025b291e2fda22327e8da6a9476e51acb1/youtokentome-1.0.6-cp36-cp36m-manylinux2010_x86_64.whl (1.7MB)\n",
            "\u001b[K     |████████████████████████████████| 1.7MB 6.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: seaborn in /usr/local/lib/python3.6/dist-packages (from -r stepik-dl-nlp/requirements.txt (line 12)) (0.10.1)\n",
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.6/dist-packages (from -r stepik-dl-nlp/requirements.txt (line 13)) (4.10.1)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.6/dist-packages (from -r stepik-dl-nlp/requirements.txt (line 14)) (5.5.0)\n",
            "Collecting pyconll\n",
            "  Downloading https://files.pythonhosted.org/packages/2c/6e/c325d0db05ac1b8d45645de903e4ba691d419e861c915c3d4ebfcaf8ac25/pyconll-2.2.1-py3-none-any.whl\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->-r stepik-dl-nlp/requirements.txt (line 1)) (0.16.0)\n",
            "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->-r stepik-dl-nlp/requirements.txt (line 1)) (1.18.5)\n",
            "Requirement already satisfied: spacy>=2.1.0 in /usr/local/lib/python3.6/dist-packages (from spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (2.2.4)\n",
            "Collecting ufal.udpipe>=1.2.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e5/72/2b8b9dc7c80017c790bb3308bbad34b57accfed2ac2f1f4ab252ff4e9cb2/ufal.udpipe-1.2.0.3.tar.gz (304kB)\n",
            "\u001b[K     |████████████████████████████████| 307kB 22.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: docopt>=0.6 in /usr/local/lib/python3.6/dist-packages (from pymorphy2->-r stepik-dl-nlp/requirements.txt (line 3)) (0.6.2)\n",
            "Collecting dawg-python>=0.7\n",
            "  Downloading https://files.pythonhosted.org/packages/6a/84/ff1ce2071d4c650ec85745766c0047ccc3b5036f1d03559fd46bb38b5eeb/DAWG_Python-0.7.2-py2.py3-none-any.whl\n",
            "Collecting pymorphy2-dicts<3.0,>=2.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/02/51/2465fd4f72328ab50877b54777764d928da8cb15b74e2680fc1bd8cb3173/pymorphy2_dicts-2.4.393442.3710985-py2.py3-none-any.whl (7.1MB)\n",
            "\u001b[K     |████████████████████████████████| 7.1MB 27.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.2->-r stepik-dl-nlp/requirements.txt (line 4)) (0.16.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->-r stepik-dl-nlp/requirements.txt (line 5)) (1.2.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->-r stepik-dl-nlp/requirements.txt (line 5)) (2.4.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->-r stepik-dl-nlp/requirements.txt (line 5)) (0.10.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->-r stepik-dl-nlp/requirements.txt (line 5)) (2.8.1)\n",
            "Collecting intervaltree>=3\n",
            "  Downloading https://files.pythonhosted.org/packages/e8/f9/76237755b2020cd74549e98667210b2dd54d3fb17c6f4a62631e61d31225/intervaltree-3.0.2.tar.gz\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->-r stepik-dl-nlp/requirements.txt (line 9)) (2018.9)\n",
            "Requirement already satisfied: Click>=7.0 in /usr/local/lib/python3.6/dist-packages (from youtokentome->-r stepik-dl-nlp/requirements.txt (line 11)) (7.1.2)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.6/dist-packages (from ipykernel->-r stepik-dl-nlp/requirements.txt (line 13)) (5.3.5)\n",
            "Requirement already satisfied: tornado>=4.0 in /usr/local/lib/python3.6/dist-packages (from ipykernel->-r stepik-dl-nlp/requirements.txt (line 13)) (4.5.3)\n",
            "Requirement already satisfied: traitlets>=4.1.0 in /usr/local/lib/python3.6/dist-packages (from ipykernel->-r stepik-dl-nlp/requirements.txt (line 13)) (4.3.3)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from ipython->-r stepik-dl-nlp/requirements.txt (line 14)) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.6/dist-packages (from ipython->-r stepik-dl-nlp/requirements.txt (line 14)) (0.7.5)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.6/dist-packages (from ipython->-r stepik-dl-nlp/requirements.txt (line 14)) (0.8.1)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.6/dist-packages (from ipython->-r stepik-dl-nlp/requirements.txt (line 14)) (1.0.18)\n",
            "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from ipython->-r stepik-dl-nlp/requirements.txt (line 14)) (4.8.0)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.6/dist-packages (from ipython->-r stepik-dl-nlp/requirements.txt (line 14)) (2.1.3)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.6/dist-packages (from ipython->-r stepik-dl-nlp/requirements.txt (line 14)) (49.1.0)\n",
            "Requirement already satisfied: requests>=2.21 in /usr/local/lib/python3.6/dist-packages (from pyconll->-r stepik-dl-nlp/requirements.txt (line 15)) (2.23.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.1.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (1.1.3)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.1.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (2.0.3)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.1.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (1.0.0)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.1.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (7.4.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.1.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (0.7.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.1.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (1.0.2)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.1.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (1.0.2)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.1.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (3.0.2)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.1.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (0.4.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from cycler>=0.10->matplotlib->-r stepik-dl-nlp/requirements.txt (line 5)) (1.12.0)\n",
            "Requirement already satisfied: sortedcontainers<3.0,>=2.0 in /usr/local/lib/python3.6/dist-packages (from intervaltree>=3->ipymarkup->-r stepik-dl-nlp/requirements.txt (line 6)) (2.2.2)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.6/dist-packages (from jupyter-client->ipykernel->-r stepik-dl-nlp/requirements.txt (line 13)) (19.0.1)\n",
            "Requirement already satisfied: jupyter-core>=4.6.0 in /usr/local/lib/python3.6/dist-packages (from jupyter-client->ipykernel->-r stepik-dl-nlp/requirements.txt (line 13)) (4.6.3)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.6/dist-packages (from traitlets>=4.1.0->ipykernel->-r stepik-dl-nlp/requirements.txt (line 13)) (0.2.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->-r stepik-dl-nlp/requirements.txt (line 14)) (0.2.5)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.6/dist-packages (from pexpect; sys_platform != \"win32\"->ipython->-r stepik-dl-nlp/requirements.txt (line 14)) (0.6.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.21->pyconll->-r stepik-dl-nlp/requirements.txt (line 15)) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.21->pyconll->-r stepik-dl-nlp/requirements.txt (line 15)) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.21->pyconll->-r stepik-dl-nlp/requirements.txt (line 15)) (2020.6.20)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.21->pyconll->-r stepik-dl-nlp/requirements.txt (line 15)) (3.0.4)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.1.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (1.7.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.1.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (3.1.0)\n",
            "Building wheels for collected packages: ufal.udpipe, intervaltree\n",
            "  Building wheel for ufal.udpipe (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ufal.udpipe: filename=ufal.udpipe-1.2.0.3-cp36-cp36m-linux_x86_64.whl size=5625176 sha256=8766e31a157bfb87faf5c14398b075ad43018032658ed2d1c9bcd7d81c73b5f9\n",
            "  Stored in directory: /root/.cache/pip/wheels/0c/9d/db/6d3404c33da5b7adb6c6972853efb6a27649d3ba15f7e9bebb\n",
            "  Building wheel for intervaltree (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for intervaltree: filename=intervaltree-3.0.2-cp36-none-any.whl size=25791 sha256=c1cc11936b0a4c80fb19575ba92af22f3900ab47aa073963d89c12ad5aa43ccc\n",
            "  Stored in directory: /root/.cache/pip/wheels/08/99/c0/5a5942f5b9567c59c14aac76f95a70bf11dccc71240b91ebf5\n",
            "Successfully built ufal.udpipe intervaltree\n",
            "Installing collected packages: ufal.udpipe, spacy-udpipe, dawg-python, pymorphy2-dicts, pymorphy2, intervaltree, ipymarkup, youtokentome, pyconll\n",
            "  Found existing installation: intervaltree 2.1.0\n",
            "    Uninstalling intervaltree-2.1.0:\n",
            "      Successfully uninstalled intervaltree-2.1.0\n",
            "Successfully installed dawg-python-0.7.2 intervaltree-3.0.2 ipymarkup-0.9.0 pyconll-2.2.1 pymorphy2-0.8 pymorphy2-dicts-2.4.393442.3710985 spacy-udpipe-0.3.1 ufal.udpipe-1.2.0.3 youtokentome-1.0.6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-09-12T12:42:57.265628Z",
          "start_time": "2019-09-12T12:42:55.188211Z"
        },
        "id": "9VZNerBrWaiF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        " \n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.metrics import accuracy_score\n",
        " \n",
        "import numpy as np\n",
        " \n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        " \n",
        "import collections\n",
        " \n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        " \n",
        "import dlnlputils\n",
        "from dlnlputils.data import tokenize_text_simple_regex, tokenize_corpus, build_vocabulary, \\\n",
        "    vectorize_texts, SparseFeaturesDataset\n",
        "from dlnlputils.pipeline import train_eval_loop, predict_with_model, init_random_seed\n",
        " \n",
        "init_random_seed()"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LTdT7tV7WaiP",
        "colab_type": "text"
      },
      "source": [
        "## Предобработка текстов и подготовка признаков"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-09-12T12:42:57.847399Z",
          "start_time": "2019-09-12T12:42:57.268037Z"
        },
        "id": "2QJtW4v7WaiR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 453
        },
        "outputId": "63ccfa39-2e8b-4986-d40d-7177eea5fe0e"
      },
      "source": [
        "train_source = fetch_20newsgroups(subset='train')\n",
        "test_source = fetch_20newsgroups(subset='test')\n",
        " \n",
        "print('Количество обучающих текстов', len(train_source['data']))\n",
        "print('Количество тестовых текстов', len(test_source['data']))\n",
        "print()\n",
        "print(train_source['data'][0].strip())\n",
        " \n",
        "print()\n",
        "print('Метка', train_source['target'][0])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading 20news dataset. This may take a few minutes.\n",
            "Downloading dataset from https://ndownloader.figshare.com/files/5975967 (14 MB)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Количество обучающих текстов 11314\n",
            "Количество тестовых текстов 7532\n",
            "\n",
            "From: lerxst@wam.umd.edu (where's my thing)\n",
            "Subject: WHAT car is this!?\n",
            "Nntp-Posting-Host: rac3.wam.umd.edu\n",
            "Organization: University of Maryland, College Park\n",
            "Lines: 15\n",
            "\n",
            " I was wondering if anyone out there could enlighten me on this car I saw\n",
            "the other day. It was a 2-door sports car, looked to be from the late 60s/\n",
            "early 70s. It was called a Bricklin. The doors were really small. In addition,\n",
            "the front bumper was separate from the rest of the body. This is \n",
            "all I know. If anyone can tellme a model name, engine specs, years\n",
            "of production, where this car is made, history, or whatever info you\n",
            "have on this funky looking car, please e-mail.\n",
            "\n",
            "Thanks,\n",
            "- IL\n",
            "   ---- brought to you by your neighborhood Lerxst ----\n",
            "\n",
            "Метка 7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eeQxeWAFWaiW",
        "colab_type": "text"
      },
      "source": [
        "### Подготовка признаков"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-09-12T12:43:00.294422Z",
          "start_time": "2019-09-12T12:42:57.849386Z"
        },
        "id": "ft4oFT1TWaiW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "772ff0bb-9ae9-45b3-833c-272a706233d7"
      },
      "source": [
        "train_tokenized = tokenize_corpus(train_source['data'])\n",
        "test_tokenized = tokenize_corpus(test_source['data'])\n",
        " \n",
        "print(' '.join(train_tokenized[0]))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "from lerxst where thing subject what this nntp posting host rac3 organization university maryland college park lines wondering anyone there could enlighten this other door sports looked from late early called bricklin doors were really small addition front bumper separate from rest body this know anyone tellme model name engine specs years production where this made history whatever info have this funky looking please mail thanks brought your neighborhood lerxst\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-09-12T12:43:00.825372Z",
          "start_time": "2019-09-12T12:43:00.297392Z"
        },
        "id": "qPmMeGVqWaib",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAX_DF = 0.8\n",
        "MIN_COUNT = 5\n",
        "vocabulary, word_doc_freq = build_vocabulary(train_tokenized, max_doc_freq=MAX_DF, min_count=MIN_COUNT)\n",
        "UNIQUE_WORDS_N = len(vocabulary)\n",
        "print('Количество уникальных токенов', UNIQUE_WORDS_N)\n",
        "print(list(vocabulary.items())[:10])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-09-12T12:43:01.524600Z",
          "start_time": "2019-09-12T12:43:00.829107Z"
        },
        "id": "bbPDK-AeWaik",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.hist(word_doc_freq, bins=20)\n",
        "plt.title('Распределение относительных частот слов')\n",
        "plt.yscale('log');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-09-12T12:44:16.094816Z",
          "start_time": "2019-09-12T12:43:01.526554Z"
        },
        "id": "HNzhq2GtWair",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "VECTORIZATION_MODE = 'tfidf'\n",
        "train_vectors = vectorize_texts(train_tokenized, vocabulary, word_doc_freq, mode=VECTORIZATION_MODE)\n",
        "test_vectors = vectorize_texts(test_tokenized, vocabulary, word_doc_freq, mode=VECTORIZATION_MODE)\n",
        "\n",
        "print('Размерность матрицы признаков обучающей выборки', train_vectors.shape)\n",
        "print('Размерность матрицы признаков тестовой выборки', test_vectors.shape)\n",
        "print()\n",
        "print('Количество ненулевых элементов в обучающей выборке', train_vectors.nnz)\n",
        "print('Процент заполненности матрицы признаков {:.2f}%'.format(train_vectors.nnz * 100 / (train_vectors.shape[0] * train_vectors.shape[1])))\n",
        "print()\n",
        "print('Количество ненулевых элементов в обучающей выборке', test_vectors.nnz)\n",
        "print('Процент заполненности матрицы признаков {:.2f}%'.format(test_vectors.nnz * 100 / (test_vectors.shape[0] * test_vectors.shape[1])))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-09-12T12:44:16.857114Z",
          "start_time": "2019-09-12T12:44:16.098773Z"
        },
        "id": "iU2E_1O6Wai0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.hist(train_vectors.data, bins=20)\n",
        "plt.title('Распределение весов признаков')\n",
        "plt.yscale('log');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zwjjibcqWai4",
        "colab_type": "text"
      },
      "source": [
        "### Распределение классов"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-09-12T12:44:16.864960Z",
          "start_time": "2019-09-12T12:44:16.859476Z"
        },
        "id": "1ZxFQJrnWai5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "UNIQUE_LABELS_N = len(set(train_source['target']))\n",
        "print('Количество уникальных меток', UNIQUE_LABELS_N)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-09-12T12:44:17.106036Z",
          "start_time": "2019-09-12T12:44:16.867310Z"
        },
        "id": "ezyFZqtdWai9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.hist(train_source['target'], bins=np.arange(0, 21))\n",
        "plt.title('Распределение меток в обучающей выборке');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-09-12T12:44:17.312198Z",
          "start_time": "2019-09-12T12:44:17.109884Z"
        },
        "scrolled": true,
        "id": "z9fjOZNxWajA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.hist(test_source['target'], bins=np.arange(0, 21))\n",
        "plt.title('Распределение меток в тестовой выборке');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tjnHABfXWajF",
        "colab_type": "text"
      },
      "source": [
        "### PyTorch Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-09-12T12:44:17.319292Z",
          "start_time": "2019-09-12T12:44:17.315074Z"
        },
        "id": "RlGe2bbSWajG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_dataset = SparseFeaturesDataset(train_vectors, train_source['target'])\n",
        "test_dataset = SparseFeaturesDataset(test_vectors, test_source['target'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8wuE5DvbWajJ",
        "colab_type": "text"
      },
      "source": [
        "## Обучение модели на PyTorch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-09-12T12:46:22.371272Z",
          "start_time": "2019-09-12T12:44:17.322178Z"
        },
        "scrolled": false,
        "id": "BYpdUIyvWajK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = nn.Linear(UNIQUE_WORDS_N, UNIQUE_LABELS_N)\n",
        "\n",
        "scheduler = lambda optim: \\\n",
        "    torch.optim.lr_scheduler.ReduceLROnPlateau(optim, patience=5, factor=0.5, verbose=True)\n",
        "\n",
        "best_val_loss, best_model = train_eval_loop(model=model,\n",
        "                                            train_dataset=train_dataset,\n",
        "                                            val_dataset=test_dataset,\n",
        "                                            criterion=F.cross_entropy,\n",
        "                                            lr=1e-1,\n",
        "                                            epoch_n=200,\n",
        "                                            batch_size=32,\n",
        "                                            l2_reg_alpha=0,\n",
        "                                            lr_scheduler_ctor=scheduler)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UfLVMJvTWajN",
        "colab_type": "text"
      },
      "source": [
        "## Оценка качества"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-09-12T12:46:25.105663Z",
          "start_time": "2019-09-12T12:46:22.373012Z"
        },
        "id": "bfWse8roWajO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_pred = predict_with_model(best_model, train_dataset)\n",
        " \n",
        "train_loss = F.cross_entropy(torch.from_numpy(train_pred),\n",
        "                             torch.from_numpy(train_source['target']).long())\n",
        " \n",
        "print('Среднее значение функции потерь на обучении', float(train_loss))\n",
        "print('Доля верных ответов', accuracy_score(train_source['target'], train_pred.argmax(-1)))\n",
        "print('Доля ошибок', 1 - accuracy_score(train_source['target'], train_pred.argmax(-1)))\n",
        "print()\n",
        " \n",
        " \n",
        " \n",
        "test_pred = predict_with_model(best_model, test_dataset)\n",
        " \n",
        "test_loss = F.cross_entropy(torch.from_numpy(test_pred),\n",
        "                            torch.from_numpy(test_source['target']).long())\n",
        " \n",
        "print('Среднее значение функции потерь на валидации', float(test_loss))\n",
        "print('Доля верных ответов', accuracy_score(test_source['target'], test_pred.argmax(-1)))\n",
        "print('Доля ошибок', 1 - accuracy_score(test_source['target'], test_pred.argmax(-1)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w3BSMmYaWajS",
        "colab_type": "text"
      },
      "source": [
        "# Альтернативная реализация на scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-09-12T12:46:31.791405Z",
          "start_time": "2019-09-12T12:46:25.107897Z"
        },
        "scrolled": false,
        "id": "aSSPJ6G9WajS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "sklearn_pipeline = Pipeline((('vect', TfidfVectorizer(tokenizer=tokenize_text_simple_regex,\n",
        "                                                      max_df=MAX_DF,\n",
        "                                                      min_df=MIN_COUNT)),\n",
        "                             ('cls', LogisticRegression())))\n",
        "sklearn_pipeline.fit(train_source['data'], train_source['target']);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AxzSlMHxWajX",
        "colab_type": "text"
      },
      "source": [
        "## Оценка качества"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-09-12T12:46:35.454567Z",
          "start_time": "2019-09-12T12:46:31.792832Z"
        },
        "id": "7nbgdYSdWajX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sklearn_train_pred = sklearn_pipeline.predict_proba(train_source['data'])\n",
        "sklearn_train_loss = F.cross_entropy(torch.from_numpy(sklearn_train_pred),\n",
        "                                                 torch.from_numpy(train_source['target']))\n",
        "print('Среднее значение функции потерь на обучении', float(sklearn_train_loss))\n",
        "print('Доля верных ответов', accuracy_score(train_source['target'], sklearn_train_pred.argmax(-1)))\n",
        "print()\n",
        " \n",
        "sklearn_test_pred = sklearn_pipeline.predict_proba(test_source['data'])\n",
        "sklearn_test_loss = F.cross_entropy(torch.from_numpy(sklearn_test_pred),\n",
        "                                                torch.from_numpy(test_source['target']))\n",
        "print('Среднее значение функции потерь на валидации', float(sklearn_test_loss))\n",
        "print('Доля верных ответов', accuracy_score(test_source['target'], sklearn_test_pred.argmax(-1)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4-3SH0x64xtw",
        "colab_type": "text"
      },
      "source": [
        "# Input Data Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9FJMEWJv7kly",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def random_texts(source, label, n):\n",
        "    \"\"\"Choose n random texts with label from source.\"\"\"\n",
        "    texts = [source['data'][t] for t in range(len(source['data'])) if source['target'][t] == label]\n",
        "    r_texts = random.sample(texts, n)\n",
        "    return(r_texts)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EJpxXLjY41CH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def print_random_texts(source, label, n):\n",
        "    \"\"\"Print n random texts with label from source.\"\"\"\n",
        "    texts = random_texts(source, label, n)\n",
        "    print('Метка: {} ({})'.format(label, source['target_names'][label]))\n",
        "    print('--------->\\n')\n",
        "    for text in texts:\n",
        "        print(text)\n",
        "        print('--------->\\n')"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F87qU6Zb424i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print_random_texts(train_source, 3, 5)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
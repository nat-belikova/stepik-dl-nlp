{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.10"
    },
    "latex_envs": {
      "LaTeX_envs_menu_present": true,
      "autoclose": false,
      "autocomplete": true,
      "bibliofile": "biblio.bib",
      "cite_by": "apalike",
      "current_citInitial": 1,
      "eqLabelWithNumbers": true,
      "eqNumInitial": 1,
      "hotkeys": {
        "equation": "Ctrl-E",
        "itemize": "Ctrl-I"
      },
      "labels_anchors": false,
      "latex_user_defs": false,
      "report_style_numbering": false,
      "user_envs_cfg": false
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "colab": {
      "name": "task2_word_embeddings.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FHUn_zBTm-hE",
        "colab_type": "text"
      },
      "source": [
        "# Word2Vec"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rejRhsyxm-hH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Если Вы запускаете ноутбук на colab или kaggle,\n",
        "# выполните следующие строчки, чтобы подгрузить библиотеку dlnlputils:\n",
        " \n",
        "!git clone https://github.com/Samsung-IT-Academy/stepik-dl-nlp.git && pip install -r stepik-dl-nlp/requirements.txt\n",
        "import sys; sys.path.append('./stepik-dl-nlp')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-10-29T19:19:30.785285Z",
          "start_time": "2019-10-29T19:19:29.542846Z"
        },
        "id": "vnwNRg3sm-hO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        " \n",
        "import random\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        " \n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        " \n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        " \n",
        "import dlnlputils\n",
        "from dlnlputils.data import tokenize_corpus, build_vocabulary, texts_to_token_ids, \\\n",
        "    PaddedSequenceDataset, Embeddings\n",
        "from dlnlputils.pipeline import train_eval_loop, predict_with_model, init_random_seed\n",
        "from dlnlputils.visualization import plot_vectors\n",
        " \n",
        "init_random_seed()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eShkF4Xbm-hV",
        "colab_type": "text"
      },
      "source": [
        "## Загрузка данных и подготовка корпуса"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-10-29T19:19:31.270503Z",
          "start_time": "2019-10-29T19:19:30.787789Z"
        },
        "id": "1gO5ABLzm-hW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Если Вы запускаете ноутбук на colab или kaggle, добавьте в начало пути ./stepik-dl-nlp\n",
        "full_dataset = list(pd.read_csv('./stepik-dl-nlp/datasets/nyt-ingredients-snapshot-2015.csv')['input'].dropna())\n",
        "random.shuffle(full_dataset)\n",
        " \n",
        "TRAIN_VAL_SPLIT = int(len(full_dataset) * 0.7)\n",
        "train_source = full_dataset[:TRAIN_VAL_SPLIT]\n",
        "test_source = full_dataset[TRAIN_VAL_SPLIT:]\n",
        "print(\"Обучающая выборка\", len(train_source))\n",
        "print(\"Тестовая выборка\", len(test_source))\n",
        "print()\n",
        "print('\\n'.join(train_source[:10]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-10-29T19:19:32.137838Z",
          "start_time": "2019-10-29T19:19:31.272363Z"
        },
        "id": "DawPT55im-hc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# токенизируем\n",
        "train_tokenized = tokenize_corpus(train_source)\n",
        "test_tokenized = tokenize_corpus(test_source)\n",
        "print('\\n'.join(' '.join(sent) for sent in train_tokenized[:10]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-10-29T19:19:32.325205Z",
          "start_time": "2019-10-29T19:19:32.140837Z"
        },
        "id": "bHQJ9VMHm-hh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# строим словарь\n",
        "vocabulary, word_doc_freq = build_vocabulary(train_tokenized, max_doc_freq=0.9, min_count=5, pad_word='<PAD>')\n",
        "print(\"Размер словаря\", len(vocabulary))\n",
        "print(list(vocabulary.items())[:10])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-10-29T19:19:32.686258Z",
          "start_time": "2019-10-29T19:19:32.327711Z"
        },
        "id": "N7q8CHB1m-hm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# отображаем в номера токенов\n",
        "train_token_ids = texts_to_token_ids(train_tokenized, vocabulary)\n",
        "test_token_ids = texts_to_token_ids(test_tokenized, vocabulary)\n",
        " \n",
        "print('\\n'.join(' '.join(str(t) for t in sent)\n",
        "                for sent in train_token_ids[:10]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-10-29T19:19:32.967989Z",
          "start_time": "2019-10-29T19:19:32.688319Z"
        },
        "id": "IdKySxmmm-hr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.hist([len(s) for s in train_token_ids], bins=20);\n",
        "plt.title('Гистограмма длин предложений');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-10-29T19:19:33.001487Z",
          "start_time": "2019-10-29T19:19:32.970153Z"
        },
        "id": "q86wa9Fym-hy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAX_SENTENCE_LEN = 20\n",
        "train_dataset = PaddedSequenceDataset(train_token_ids,\n",
        "                                      np.zeros(len(train_token_ids)),\n",
        "                                      out_len=MAX_SENTENCE_LEN)\n",
        "test_dataset = PaddedSequenceDataset(test_token_ids,\n",
        "                                     np.zeros(len(test_token_ids)),\n",
        "                                     out_len=MAX_SENTENCE_LEN)\n",
        "print(train_dataset[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZt9ZAyDm-h2",
        "colab_type": "text"
      },
      "source": [
        "## Алгоритм обучения - Skip Gram Negative Sampling\n",
        "\n",
        "**Skip Gram** - предсказываем соседние слова по центральному слову\n",
        "\n",
        "**Negative Sampling** - аппроксимация softmax\n",
        "\n",
        "$$ W, D \\in \\mathbb{R}^{Vocab \\times EmbSize} $$\n",
        "\n",
        "$$ \\sum_{CenterW_i} P(CtxW_{-2}, CtxW_{-1}, CtxW_{+1}, CtxW_{+2} | CenterW_i; W, D) \\rightarrow \\max_{W,D} $$\n",
        "\n",
        "$$ P(CtxW_{-2}, CtxW_{-1}, CtxW_{+1}, CtxW_{+2} | CenterW_i; W, D) = \\prod_j P(CtxW_j | CenterW_i; W, D) $$\n",
        "    \n",
        "$$ P(CtxW_j | CenterW_i; W, D) = \\frac{e^{w_i \\cdot d_j}} { \\sum_{j=1}^{|V|} e^{w_i \\cdot d_j}} = softmax \\simeq \\frac{e^{w_i \\cdot d_j^+}} { \\sum_{j=1}^{k} e^{w_i \\cdot d_j^-}}, \\quad k \\ll |V| $$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-10-29T19:19:33.065376Z",
          "start_time": "2019-10-29T19:19:33.003081Z"
        },
        "id": "1mF4x0Ilm-h3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def make_diag_mask(size, radius):\n",
        "    \"\"\"Квадратная матрица размера Size x Size с двумя полосами ширины radius вдоль главной диагонали\"\"\"\n",
        "    idxs = torch.arange(size)\n",
        "    abs_idx_diff = (idxs.unsqueeze(0) - idxs.unsqueeze(1)).abs()\n",
        "    mask = ((abs_idx_diff <= radius) & (abs_idx_diff > 0)).float()\n",
        "    return mask\n",
        " \n",
        "make_diag_mask(10, 3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nTslNsaJm-h6",
        "colab_type": "text"
      },
      "source": [
        "**Negative Sampling** работает следующим образом - мы **максимизируем сумму вероятностей двух событий**: \n",
        "\n",
        "* \"этот пример центрального слова вместе с контекстными словами взят **из тренировочной выборки**\": $$ P(y=1 | CenterW_i; CtxW_j) = sigmoid(w_i \\cdot d_j) = \\frac{1}{1+e^{-w_i \\cdot d_j}} $$\n",
        "\n",
        "$$ \\\\ $$\n",
        "\n",
        "* \"этот пример центрального слова вместе со случайми контекстными словами **выдуман** \": $$ P(y=0 | CenterW_i; CtxW_{noise}) = 1 - P(y=1 | CenterW_i;  CtxW_{noise}) = \\frac{1}{1+e^{w_i \\cdot d_{noise}}} $$\n",
        "\n",
        "$$ \\\\ $$\n",
        "\n",
        "$$ NEG(CtxW_j, CenterW_i) = log(\\frac{1}{1+e^{-w_i \\cdot d_j}}) + \\sum_{l=1}^{k}log(\\frac{1}{1+e^{w_i \\cdot d_{noise_l}}})  \\rightarrow \\max_{W,D} $$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-10-29T19:19:33.101379Z",
          "start_time": "2019-10-29T19:19:33.068154Z"
        },
        "id": "y9kiEy8Qm-h7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SkipGramNegativeSamplingTrainer(nn.Module):\n",
        "    def __init__(self, vocab_size, emb_size, sentence_len, radius=5, negative_samples_n=5):\n",
        "        super().__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.negative_samples_n = negative_samples_n\n",
        " \n",
        "        self.center_emb = nn.Embedding(self.vocab_size, emb_size, padding_idx=0)\n",
        "        self.center_emb.weight.data.uniform_(-1.0 / emb_size, 1.0 / emb_size)\n",
        "        self.center_emb.weight.data[0] = 0\n",
        " \n",
        "        self.context_emb = nn.Embedding(self.vocab_size, emb_size, padding_idx=0)        \n",
        "        self.context_emb.weight.data.uniform_(-1.0 / emb_size, 1.0 / emb_size)\n",
        "        self.context_emb.weight.data[0] = 0\n",
        " \n",
        "        self.positive_sim_mask = make_diag_mask(sentence_len, radius)\n",
        "    \n",
        "    def forward(self, sentences):\n",
        "        \"\"\"sentences - Batch x MaxSentLength - идентификаторы токенов\"\"\"\n",
        "        batch_size = sentences.shape[0]\n",
        "        center_embeddings = self.center_emb(sentences)  # Batch x MaxSentLength x EmbSize\n",
        " \n",
        "        # оценить сходство с настоящими соседними словами\n",
        "        positive_context_embs = self.context_emb(sentences).permute(0, 2, 1)  # Batch x EmbSize x MaxSentLength\n",
        "        positive_sims = torch.bmm(center_embeddings, positive_context_embs)  # Batch x MaxSentLength x MaxSentLength\n",
        "        positive_probs = torch.sigmoid(positive_sims)\n",
        " \n",
        "        # увеличить оценку вероятности встретить эти пары слов вместе\n",
        "        positive_mask = self.positive_sim_mask.to(positive_sims.device)\n",
        "        positive_loss = F.binary_cross_entropy(positive_probs * positive_mask,\n",
        "                                               positive_mask.expand_as(positive_probs))\n",
        " \n",
        "        # выбрать случайные \"отрицательные\" слова\n",
        "        negative_words = torch.randint(1, self.vocab_size,\n",
        "                                       size=(batch_size, self.negative_samples_n),\n",
        "                                       device=sentences.device)  # Batch x NegSamplesN\n",
        "        negative_context_embs = self.context_emb(negative_words).permute(0, 2, 1)  # Batch x EmbSize x NegSamplesN\n",
        "        negative_sims = torch.bmm(center_embeddings, negative_context_embs)  # Batch x MaxSentLength x NegSamplesN\n",
        "        \n",
        "        # уменьшить оценку вероятность встретить эти пары слов вместе\n",
        "        negative_loss = F.binary_cross_entropy_with_logits(negative_sims,\n",
        "                                                           negative_sims.new_zeros(negative_sims.shape))\n",
        " \n",
        "        return positive_loss + negative_loss\n",
        " \n",
        " \n",
        "def no_loss(pred, target):\n",
        "    \"\"\"Фиктивная функция потерь - когда модель сама считает функцию потерь\"\"\"\n",
        "    return pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VuYvKS1Pm-iA",
        "colab_type": "text"
      },
      "source": [
        "## Обучение"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-10-29T19:19:33.130307Z",
          "start_time": "2019-10-29T19:19:33.103036Z"
        },
        "id": "Er_J1iDMm-iA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trainer = SkipGramNegativeSamplingTrainer(len(vocabulary), 100, MAX_SENTENCE_LEN,\n",
        "                                          radius=5, negative_samples_n=25)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-10-29T19:20:12.830221Z",
          "start_time": "2019-10-29T19:19:33.132062Z"
        },
        "scrolled": false,
        "id": "3uM2m3lPm-iI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "best_val_loss, best_model = train_eval_loop(trainer,\n",
        "                                            train_dataset,\n",
        "                                            test_dataset,\n",
        "                                            no_loss,\n",
        "                                            lr=1e-2,\n",
        "                                            epoch_n=2,\n",
        "                                            batch_size=8,\n",
        "                                            device='cpu',\n",
        "                                            early_stopping_patience=10,\n",
        "                                            max_batches_per_epoch_train=2000,\n",
        "                                            max_batches_per_epoch_val=len(test_dataset),\n",
        "                                            lr_scheduler_ctor=lambda optim: torch.optim.lr_scheduler.ReduceLROnPlateau(optim, patience=1, verbose=True))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-10-29T19:20:12.862018Z",
          "start_time": "2019-10-29T19:20:12.832046Z"
        },
        "id": "uqcjVh7Hm-iM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Если Вы запускаете ноутбук на colab или kaggle, добавьте в начало пути ./stepik-dl-nlp\n",
        "torch.save(trainer.state_dict(), './stepik-dl-nlp/models/sgns.pth')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-10-29T19:20:12.888270Z",
          "start_time": "2019-10-29T19:20:12.864706Z"
        },
        "id": "DDFA9ue4m-iQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Если Вы запускаете ноутбук на colab или kaggle, добавьте в начало пути ./stepik-dl-nlp\n",
        "trainer.load_state_dict(torch.load('./stepik-dl-nlp/models/sgns.pth'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iNksGDa7m-iT",
        "colab_type": "text"
      },
      "source": [
        "## Исследуем характеристики полученных векторов"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-10-29T19:20:12.919904Z",
          "start_time": "2019-10-29T19:20:12.890671Z"
        },
        "id": "FfWVicXzm-iU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embeddings = Embeddings(trainer.center_emb.weight.detach().cpu().numpy(), vocabulary)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-10-29T19:20:12.942708Z",
          "start_time": "2019-10-29T19:20:12.921619Z"
        },
        "id": "Av3YxE5Lm-iX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embeddings.most_similar('chicken')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-10-29T19:20:12.965936Z",
          "start_time": "2019-10-29T19:20:12.944423Z"
        },
        "id": "BrmWZASbm-ic",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embeddings.analogy('cake', 'cacao', 'cheese')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-10-29T19:20:12.991060Z",
          "start_time": "2019-10-29T19:20:12.967532Z"
        },
        "id": "-aenAjAnm-ij",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_words = ['salad', 'fish', 'salmon', 'sauvignon', 'beef', 'pork', 'steak', 'beer', 'cake', 'coffee', 'sausage', 'wine', 'merlot', 'zinfandel', 'trout', 'chardonnay', 'champagne', 'cacao']\n",
        "test_vectors = embeddings.get_vectors(*test_words)\n",
        "print(test_vectors.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-10-29T19:20:13.318676Z",
          "start_time": "2019-10-29T19:20:12.996595Z"
        },
        "id": "C0csX7CDm-iu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig, ax = plt.subplots()\n",
        "fig.set_size_inches((10, 10))\n",
        "plot_vectors(test_vectors, test_words, how='svd', ax=ax)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pB2TavcAIGqN",
        "colab_type": "text"
      },
      "source": [
        "## Model Quality"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pL3P8ywPI_Sn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " # print sentences containing specific token\n",
        "t = [train_source[i] for i in range(len(train_source)) if 'noodles' in train_tokenized[i]]\n",
        "print('\\n'.join(t[:10]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gg4LnxtCJAjt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " # print sentences containing two specific tokens\n",
        "t = [train_source[i] for i in range(len(train_source)) if 'chopped' in train_tokenized[i] and 'flat' in train_tokenized[i]]\n",
        "print('\\n'.join(t[:10]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "54xDjYwgIXYD",
        "colab_type": "text"
      },
      "source": [
        "### Baseline Model: Top-5 Most Similar Words \n",
        "salt: kosher, flaky, taste, fleur, black (1/5)\n",
        " \n",
        "piece: inches, root, about, length, candied (0/5)\n",
        " \n",
        "fresh: snipped, eyed, dried, freshly, branches (1/5)\n",
        " \n",
        "coarsely: chopped, roughly, finely, mangoes, halved (3/5)\n",
        " \n",
        "chopped: minced, finely, coarsely, roughly, flat (1/5)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vlZovPBfIhdd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ms_list = ['salt', 'piece', 'fresh', 'coarsely', 'chopped']\n",
        "for el in ms_list:\n",
        "    for em in embeddings.most_similar(el, topk=6):\n",
        "        print(em)\n",
        "    print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tzfPzHD0ImNC",
        "colab_type": "text"
      },
      "source": [
        "### Baseline Model: Analogy \n",
        "seeds : chia = leaves : ?\n",
        "leafy, branches, masa, leaves, branch (0/5)\n",
        " \n",
        "fish : salmon = wine : ?\n",
        "wine, salmon, champagne, shallots, noodles (1/5)\n",
        " \n",
        "black : pepper = yellow : ?\n",
        "yellow, pepper, peppers, seeded, onion (2/5)\n",
        " \n",
        "quart : milk = teaspoon : ?\n",
        "teaspoon, tablespoon, teaspoons, milk, flour (2/5)\n",
        " \n",
        "sliced : thinly = minced : ?\n",
        "minced, thinly, dill, chopped, jalapeño (1/5)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0MSeNiamItoe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "a_list = [('seeds', 'chia', 'leaves'),\n",
        "    ('fish', 'salmon', 'wine'),\n",
        "    ('black', 'pepper', 'yellow'),\n",
        "    ('quart', 'milk', 'teaspoon'),\n",
        "    ('sliced', 'thinly', 'minced')\n",
        "    ]\n",
        "for a in a_list:\n",
        "    print(a)\n",
        "    for v in embeddings.analogy(*a, topk=5):\n",
        "        print(v)\n",
        "    print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pK99Adkem-ix",
        "colab_type": "text"
      },
      "source": [
        "## Обучение Word2Vec с помощью Gensim"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-10-29T19:20:13.613797Z",
          "start_time": "2019-10-29T19:20:13.321353Z"
        },
        "id": "K56GQCNSm-ix",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gensim"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-10-29T19:20:17.075005Z",
          "start_time": "2019-10-29T19:20:13.615729Z"
        },
        "id": "l63IOLJEm-i0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word2vec = gensim.models.Word2Vec(sentences=train_tokenized, size=100,\n",
        "                                  window=5, min_count=5, workers=4,\n",
        "                                  sg=1, iter=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-10-29T19:20:17.109583Z",
          "start_time": "2019-10-29T19:20:17.076599Z"
        },
        "id": "7b3o8l22m-i3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word2vec.wv.most_similar('chicken')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-10-29T19:20:17.176357Z",
          "start_time": "2019-10-29T19:20:17.112948Z"
        },
        "id": "EbluZX-um-i6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gensim_words = [w for w in test_words if w in word2vec.wv.vocab]\n",
        "gensim_vectors = np.stack([word2vec.wv[w] for w in gensim_words])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-10-29T19:20:17.428874Z",
          "start_time": "2019-10-29T19:20:17.179311Z"
        },
        "id": "PDEjSVAbm-i8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig, ax = plt.subplots()\n",
        "fig.set_size_inches((10, 10))\n",
        "plot_vectors(gensim_vectors, test_words, how='svd', ax=ax)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lW4Yst-Jm-jB",
        "colab_type": "text"
      },
      "source": [
        "## Загрузка предобученного Word2Vec\n",
        "\n",
        "Источники готовых векторов:\n",
        "\n",
        "https://rusvectores.org/ru/ - для русского языка\n",
        "\n",
        "https://wikipedia2vec.github.io/wikipedia2vec/pretrained/ - много разных языков"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-10-29T19:20:17.460133Z",
          "start_time": "2019-10-29T19:20:17.430563Z"
        },
        "id": "TGXHQLK_m-jB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gensim.downloader as api"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-10-29T19:20:17.980509Z",
          "start_time": "2019-10-29T19:20:17.462239Z"
        },
        "id": "QyOlW9lGm-jM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "available_models = api.info()['models'].keys()\n",
        "print('\\n'.join(available_models))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-10-29T19:22:12.649035Z",
          "start_time": "2019-10-29T19:20:17.984118Z"
        },
        "scrolled": false,
        "id": "LujCEw8lm-jP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pretrained = api.load('word2vec-google-news-300')  # > 1.5 GB!"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-10-29T19:22:12.651388Z",
          "start_time": "2019-10-29T19:19:29.817Z"
        },
        "id": "Zp2haU7nm-jS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pretrained.most_similar('cheese')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-10-29T19:22:12.652649Z",
          "start_time": "2019-10-29T19:19:29.820Z"
        },
        "id": "CM5sfNL1m-jV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pretrained.most_similar(positive=['man', 'queen'], negative=['king'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-10-29T19:22:12.653584Z",
          "start_time": "2019-10-29T19:19:29.823Z"
        },
        "id": "0OxuHRCCm-jb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pretrained_words = [w for w in test_words if w in pretrained.vocab]\n",
        "pretrained_vectors = np.stack([pretrained[w] for w in pretrained_words])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-10-29T19:22:12.654594Z",
          "start_time": "2019-10-29T19:19:29.828Z"
        },
        "id": "O8RuufX8m-je",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig, ax = plt.subplots()\n",
        "fig.set_size_inches((10, 10))\n",
        "plot_vectors(pretrained_vectors, test_words, how='svd', ax=ax)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i_140sDUm-jh",
        "colab_type": "text"
      },
      "source": [
        "## Заключение\n",
        "\n",
        "* Реализовали Skip Gram Negative Sampling на PyTorch\n",
        "* Обучили на корпусе рецептов\n",
        "    * Сходство слов модель выучила неплохо\n",
        "    * Для аналогий мало данных\n",
        "* Обучили SGNS с помощью библиотеки Gensim\n",
        "* Загрузили веса Word2Vec, полученные с помощью большого корпуса (GoogleNews)\n",
        "    * Списки похожих слов отличаются!\n",
        "    * Аналогии работают"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p90ZRYVnm-jh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
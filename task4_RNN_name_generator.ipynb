{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "latex_envs": {
      "LaTeX_envs_menu_present": true,
      "autoclose": false,
      "autocomplete": true,
      "bibliofile": "biblio.bib",
      "cite_by": "apalike",
      "current_citInitial": 1,
      "eqLabelWithNumbers": true,
      "eqNumInitial": 1,
      "hotkeys": {
        "equation": "Ctrl-E",
        "itemize": "Ctrl-I"
      },
      "labels_anchors": false,
      "latex_user_defs": false,
      "report_style_numbering": false,
      "user_envs_cfg": false
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "colab": {
      "name": "task4_RNN_name_generator.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9xbuZBO-eonW",
        "colab_type": "text"
      },
      "source": [
        "# Генерация текста с помощью RNN\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZWs_gKpLeonY",
        "colab_type": "text"
      },
      "source": [
        "(по мотивам [семинара](https://github.com/neychev/harbour_dlia2019/blob/master/day02_Simple_RNN/Day_2_Simple_RNN_pytorch.ipynb)\n",
        " [курса \"Deep Learning in Applications\"](https://in.harbour.space/data-science/deep-learning-in-applications-radoslav-neychev-anastasia-ianina/))"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DT3vdOSpeonZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Если Вы запускаете ноутбук на colab или kaggle,\n",
        "# выполните следующие строчки, чтобы подгрузить библиотеку dlnlputils:\n",
        " \n",
        "!git clone https://github.com/Samsung-IT-Academy/stepik-dl-nlp.git && pip install -r stepik-dl-nlp/requirements.txt\n",
        "import sys; sys.path.append('./stepik-dl-nlp')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-11-05T18:20:34.854793Z",
          "start_time": "2019-11-05T18:20:34.372865Z"
        },
        "id": "eKBjOagSeonf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "beGNVFeVeonp",
        "colab_type": "text"
      },
      "source": [
        "# Данные\n",
        "Датасет содержит ~9k имен, все написаны латиницей."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-11-05T18:21:03.509714Z",
          "start_time": "2019-11-05T18:21:03.491489Z"
        },
        "id": "3NAYfonveonq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Если Вы запускаете ноутбук на colab или kaggle, добавьте в начало пути ./stepik-dl-nlp\n",
        "with open('./stepik-dl-nlp/datasets/russian_names.txt') as input_file:\n",
        "    names = input_file.read()[:-1].split('\\n')\n",
        "    names = [' ' + line for line in names]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-11-05T18:21:03.946758Z",
          "start_time": "2019-11-05T18:21:03.938432Z"
        },
        "id": "t8--V0kpeony",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "names[:5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "scsuUtX7eon3",
        "colab_type": "text"
      },
      "source": [
        "Посмотрим на распределение длин имен:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-11-05T18:21:05.420060Z",
          "start_time": "2019-11-05T18:21:05.179513Z"
        },
        "id": "djm4JZHkeon4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.title('Name length distribution')\n",
        "plt.hist(list(map(len, names)), bins=25);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cejk3R6aeon8",
        "colab_type": "text"
      },
      "source": [
        "# Препроцессинг"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-11-05T18:21:07.335188Z",
          "start_time": "2019-11-05T18:21:07.320148Z"
        },
        "id": "sR1p92UEeon9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#all unique characters go here\n",
        "tokens = list(set(''.join(names)))\n",
        "\n",
        "num_tokens = len(tokens)\n",
        "print ('num_tokens = ', num_tokens)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3c_EgChyeooC",
        "colab_type": "text"
      },
      "source": [
        "### Символы -> id\n",
        "\n",
        "Создадим словарь < символ > -> < id >"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-11-05T18:21:07.674548Z",
          "start_time": "2019-11-05T18:21:07.671129Z"
        },
        "id": "XjJ4xupZeooD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "token_to_id = {token: idx for idx, token in enumerate(tokens)}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-11-05T18:21:07.838814Z",
          "start_time": "2019-11-05T18:21:07.833611Z"
        },
        "id": "g_X_UrRreooI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "assert len(tokens) == len(token_to_id), \"dictionaries must have same size\"\n",
        "\n",
        "for i in range(num_tokens):\n",
        "    assert token_to_id[tokens[i]] == i, \"token identifier must be it's position in tokens list\"\n",
        "\n",
        "print(\"Seems alright!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-11-05T18:21:07.988093Z",
          "start_time": "2019-11-05T18:21:07.977722Z"
        },
        "id": "a0N5yKwueooO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def to_matrix(data, token_to_id, max_len=None, dtype='int32', batch_first = True):\n",
        "    \"\"\"Casts a list of names into rnn-digestable matrix\"\"\"\n",
        "    \n",
        "    max_len = max_len or max(map(len, data))\n",
        "    data_ix = np.zeros([len(data), max_len], dtype) + token_to_id[' ']\n",
        "\n",
        "    for i in range(len(data)):\n",
        "        line_ix = [token_to_id[c] for c in data[i]]\n",
        "        data_ix[i, :len(line_ix)] = line_ix\n",
        "        \n",
        "    if not batch_first: # convert [batch, time] into [time, batch]\n",
        "        data_ix = np.transpose(data_ix)\n",
        "\n",
        "    return data_ix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-11-05T18:21:08.136936Z",
          "start_time": "2019-11-05T18:21:08.131609Z"
        },
        "id": "sxzwzzW1eooR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Example: cast 4 names to matrices, pad with zeros\n",
        "print('\\n'.join(names[::2000]))\n",
        "print(to_matrix(names[::2000], token_to_id))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FSA4vBoNeooW",
        "colab_type": "text"
      },
      "source": [
        "# Рекуррентные нейронные сети\n",
        "\n",
        "<img src=\"https://github.com/nat-belikova/stepik-dl-nlp/blob/master/img/rnn.png?raw=1\" width=480>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-11-05T18:21:10.739438Z",
          "start_time": "2019-11-05T18:21:09.661222Z"
        },
        "id": "mBlpAqeBeooX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch, torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-11-05T18:21:10.751862Z",
          "start_time": "2019-11-05T18:21:10.741772Z"
        },
        "id": "ThgEatYAeooa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CharRNNCell(nn.Module):\n",
        "    \"\"\"\n",
        "    Implement the scheme above as torch module\n",
        "    \"\"\"\n",
        "    def __init__(self, num_tokens=len(tokens), embedding_size=16, rnn_num_units=64):\n",
        "        super(self.__class__,self).__init__()\n",
        "        self.num_units = rnn_num_units\n",
        "        \n",
        "        self.embedding = nn.Embedding(num_tokens, embedding_size)\n",
        "        self.rnn_update = nn.Linear(embedding_size + rnn_num_units, rnn_num_units)\n",
        "        self.rnn_to_logits = nn.Linear(rnn_num_units, num_tokens)\n",
        "        \n",
        "    def forward(self, x, h_prev):\n",
        "        \"\"\"\n",
        "        This method computes h_next(x, h_prev) and log P(x_next | h_next)\n",
        "        We'll call it repeatedly to produce the whole sequence.\n",
        "        \n",
        "        :param x: batch of character ids, variable containing vector of int64\n",
        "        :param h_prev: previous rnn hidden states, variable containing matrix [batch, rnn_num_units] of float32\n",
        "        \"\"\"\n",
        "        # get vector embedding of x\n",
        "        x_emb = self.embedding(x)\n",
        "        \n",
        "        # compute next hidden state using self.rnn_update\n",
        "        x_and_h = torch.cat([x_emb, h_prev], dim=1) #YOUR CODE HERE\n",
        "        h_next = self.rnn_update(x_and_h) #YOUR CODE HERE\n",
        "        \n",
        "        h_next = F.tanh(h_next)\n",
        "        \n",
        "        assert h_next.size() == h_prev.size()\n",
        "        \n",
        "        #compute logits for next character probs\n",
        "        logits = self.rnn_to_logits(h_next)\n",
        "        \n",
        "        return h_next, F.log_softmax(logits, -1)\n",
        "    \n",
        "    def initial_state(self, batch_size):\n",
        "        \"\"\" return rnn state before it processes first input (aka h0) \"\"\"\n",
        "        return Variable(torch.zeros(batch_size, self.num_units))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-11-05T18:21:11.071002Z",
          "start_time": "2019-11-05T18:21:11.052377Z"
        },
        "id": "LVieG4Aieoog",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "char_rnn = CharRNNCell()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xBoZIu4feool",
        "colab_type": "text"
      },
      "source": [
        "### Тренировка сети, RNN loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-11-05T18:21:11.521078Z",
          "start_time": "2019-11-05T18:21:11.510175Z"
        },
        "id": "Ve5KrPzZeoom",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def rnn_loop(rnn, batch_index):\n",
        "    \"\"\"\n",
        "    Computes log P(next_character) for all time-steps in names_ix\n",
        "    :param names_ix: an int32 matrix of shape [batch, time], output of to_matrix(names)\n",
        "    \"\"\"\n",
        "    batch_size, max_length = batch_index.size()\n",
        "    hid_state = rnn.initial_state(batch_size)\n",
        "    logprobs = []\n",
        "\n",
        "    for x_t in batch_index.transpose(0,1):\n",
        "        hid_state, logp_next = rnn(x_t, hid_state)  \n",
        "        logprobs.append(logp_next)\n",
        "        \n",
        "    return torch.stack(logprobs, dim=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KJimFoWXeoop",
        "colab_type": "text"
      },
      "source": [
        "### Тренировка сети"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-11-05T18:21:12.120106Z",
          "start_time": "2019-11-05T18:21:12.109585Z"
        },
        "id": "Iz6_k5Bteooq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from IPython.display import clear_output\n",
        "from random import sample\n",
        "\n",
        "char_rnn = CharRNNCell()\n",
        "opt = torch.optim.Adam(char_rnn.parameters())\n",
        "history = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-11-05T18:21:23.521061Z",
          "start_time": "2019-11-05T18:21:12.302892Z"
        },
        "id": "OVzZeHOceoou",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAX_LENGTH = max(map(len, names))\n",
        "\n",
        "for i in range(1000):\n",
        "    batch_ix = to_matrix(sample(names, 32), token_to_id, max_len=MAX_LENGTH)\n",
        "    batch_ix = torch.tensor(batch_ix, dtype=torch.int64)\n",
        "    \n",
        "    logp_seq = rnn_loop(char_rnn, batch_ix)\n",
        "    \n",
        "    # compute loss\n",
        "    predictions_logp = logp_seq[:, :-1]\n",
        "    actual_next_tokens = batch_ix[:, 1:]\n",
        "\n",
        "    loss = -torch.mean(torch.gather(predictions_logp, dim=2, index=actual_next_tokens[:,:,None]))###YOUR CODE\n",
        "    \n",
        "    # train with backprop\n",
        "    loss.backward()\n",
        "    opt.step()\n",
        "    opt.zero_grad()\n",
        "    \n",
        "    # visualizing training process\n",
        "    history.append(loss.data.numpy())\n",
        "    if (i + 1) % 100 == 0:\n",
        "        clear_output(True)\n",
        "        plt.plot(history,label='loss')\n",
        "        plt.legend()\n",
        "        plt.show()\n",
        "\n",
        "assert np.mean(history[:10]) > np.mean(history[-10:]), \"RNN didn't converge.\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_F__c3W5eooy",
        "colab_type": "text"
      },
      "source": [
        "### RNN: генерация имен"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-11-05T18:21:23.540765Z",
          "start_time": "2019-11-05T18:21:23.524503Z"
        },
        "id": "APdMkM2beoo0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_sample(char_rnn, seed_phrase=' ', max_length=MAX_LENGTH, temperature=1.0):\n",
        "    '''\n",
        "    The function generates text given a phrase of length at least SEQ_LENGTH.\n",
        "    :param seed_phrase: prefix characters. The RNN is asked to continue the phrase\n",
        "    :param max_length: maximum output length, including seed_phrase\n",
        "    :param temperature: coefficient for sampling.  higher temperature produces more chaotic outputs,\n",
        "                        smaller temperature converges to the single most likely output\n",
        "    '''\n",
        "    \n",
        "    x_sequence = [token_to_id[token] for token in seed_phrase]\n",
        "    x_sequence = torch.tensor([x_sequence], dtype=torch.int64)\n",
        "    hid_state = char_rnn.initial_state(batch_size=1)\n",
        "    \n",
        "    #feed the seed phrase, if any\n",
        "    for i in range(len(seed_phrase) - 1):\n",
        "        hid_state, _ = char_rnn(x_sequence[:, i], hid_state)\n",
        "    \n",
        "    #start generating\n",
        "    for _ in range(max_length - len(seed_phrase)):\n",
        "        hid_state, logp_next = char_rnn(x_sequence[:, -1], hid_state)\n",
        "        p_next = F.softmax(logp_next / temperature, dim=-1).data.numpy()[0]\n",
        "        \n",
        "        # sample next token and push it back into x_sequence\n",
        "        next_ix = np.random.choice(len(tokens), p=p_next)\n",
        "        next_ix = torch.tensor([[next_ix]], dtype=torch.int64)\n",
        "        x_sequence = torch.cat([x_sequence, next_ix], dim=1)\n",
        "        \n",
        "    return ''.join([tokens[ix] for ix in x_sequence.data.numpy()[0]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-11-05T18:21:23.625562Z",
          "start_time": "2019-11-05T18:21:23.544968Z"
        },
        "id": "5euH49abeoo3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for _ in range(10):\n",
        "    print(generate_sample(char_rnn))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-11-05T18:21:23.702249Z",
          "start_time": "2019-11-05T18:21:23.629226Z"
        },
        "id": "qNkWQIQZeoo6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for _ in range(10):\n",
        "    print(generate_sample(char_rnn, seed_phrase=' Ar'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "u-bYoO4Zeoo9",
        "colab_type": "text"
      },
      "source": [
        "### Более простое решение\n",
        "\n",
        "* `nn.RNNCell(emb_size, rnn_num_units)` - шаг RNN. Алгоритм: concat-linear-tanh\n",
        "* `nn.RNN(emb_size, rnn_num_units` - весь rnn_loop.\n",
        "\n",
        "Кроме того, в PyTorch есть `nn.LSTMCell`, `nn.LSTM`, `nn.GRUCell`, `nn.GRU`, etc. etc.\n",
        "\n",
        "Перепишем наш пример с генерацией имен с помощью средств PyTorch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-11-05T18:21:23.713285Z",
          "start_time": "2019-11-05T18:21:23.704755Z"
        },
        "id": "Hc5F1E4Peoo-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CharRNNLoop(nn.Module):\n",
        "    def __init__(self, num_tokens=num_tokens, emb_size=16, rnn_num_units=64):\n",
        "        super(self.__class__, self).__init__()\n",
        "        self.emb = nn.Embedding(num_tokens, emb_size)\n",
        "        self.rnn = nn.RNN(emb_size, rnn_num_units, batch_first=True)\n",
        "        self.hid_to_logits = nn.Linear(rnn_num_units, num_tokens)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        assert isinstance(x, Variable) and isinstance(x.data, torch.LongTensor)\n",
        "        h_seq, _ = self.rnn(self.emb(x))\n",
        "        next_logits = self.hid_to_logits(h_seq)\n",
        "        next_logp = F.log_softmax(next_logits, dim=-1)\n",
        "        return next_logp\n",
        "    \n",
        "model = CharRNNLoop()\n",
        "opt = torch.optim.Adam(model.parameters())\n",
        "history = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-11-05T18:21:23.790047Z",
          "start_time": "2019-11-05T18:21:23.715167Z"
        },
        "id": "mjJ2W39yeopB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# the model applies over the whole sequence\n",
        "batch_ix = to_matrix(sample(names, 32), token_to_id, max_len=MAX_LENGTH)\n",
        "batch_ix = Variable(torch.LongTensor(batch_ix))\n",
        "\n",
        "logp_seq = model(batch_ix)\n",
        "\n",
        "# compute loss\n",
        "loss = F.nll_loss(logp_seq[:, 1:].contiguous().view(-1, num_tokens), \n",
        "                  batch_ix[:, :-1].contiguous().view(-1))\n",
        "\n",
        "loss.backward()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-11-05T18:21:31.468107Z",
          "start_time": "2019-11-05T18:21:23.792092Z"
        },
        "id": "cjPN8fD4eopE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAX_LENGTH = max(map(len, names))\n",
        "\n",
        "for i in range(1000):\n",
        "    batch_ix = to_matrix(sample(names, 32), token_to_id, max_len=MAX_LENGTH)\n",
        "    batch_ix = torch.tensor(batch_ix, dtype=torch.int64)\n",
        "    \n",
        "    logp_seq = model(batch_ix)\n",
        "    \n",
        "    # compute loss\n",
        "    predictions_logp = logp_seq[:, :-1]\n",
        "    actual_next_tokens = batch_ix[:, 1:]\n",
        "\n",
        "    loss = -torch.mean(torch.gather(predictions_logp, dim=2, index=actual_next_tokens[:,:,None]))\n",
        "    \n",
        "    # train with backprop\n",
        "    loss.backward()\n",
        "    opt.step()\n",
        "    opt.zero_grad()\n",
        "    \n",
        "    history.append(loss.data.numpy())\n",
        "    if (i + 1) % 100 == 0:\n",
        "        clear_output(True)\n",
        "        plt.plot(history, label='loss')\n",
        "        plt.legend()\n",
        "        plt.show()\n",
        "\n",
        "assert np.mean(history[:25]) > np.mean(history[-25:]), \"RNN didn't converge.\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-11-05T18:21:31.526436Z",
          "start_time": "2019-11-05T18:21:31.469965Z"
        },
        "id": "uG3Rk942eopI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for _ in range(10):\n",
        "    print(generate_sample(char_rnn))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bglqxEi3eopO",
        "colab_type": "text"
      },
      "source": [
        "### Домашнее задание: мотивационные лозунги"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-11-05T18:21:31.570320Z",
          "start_time": "2019-11-05T18:21:31.528673Z"
        },
        "id": "OfI_FIu8eopP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Если Вы запускаете ноутбук на colab или kaggle, добавьте в начало пути ./stepik-dl-nlp\n",
        "with open('./stepik-dl-nlp/datasets/author_quotes.txt') as input_file:\n",
        "    quotes = input_file.read()[:-1].split('\\n')\n",
        "    quotes = [' ' + line for line in quotes]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-11-05T18:21:31.575286Z",
          "start_time": "2019-11-05T18:21:31.571798Z"
        },
        "id": "9geS_ONKeopS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "quotes[:5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-11-05T18:21:31.653673Z",
          "start_time": "2019-11-05T18:21:31.578424Z"
        },
        "id": "CSu0bN9feopV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokens = list(set(''.join(quotes)))\n",
        "token_to_id = {token: idx for idx, token in enumerate(tokens)}\n",
        "num_tokens = len(tokens)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O3EIuo6Geopa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lXFzlbPUeope",
        "colab_type": "text"
      },
      "source": [
        "### Что еще можно генерировать?\n",
        "С помощью кода из этого семинара можно генерировать не только имена, но и:\n",
        "\n",
        "* Повести/романы/поэзию/песни любимого автора\n",
        "* Новостные заголовки\n",
        "* Программный код\n",
        "* Молекулы в формате [smiles](https://en.wikipedia.org/wiki/Simplified_molecular-input_line-entry_system)\n",
        "* Музыку\n",
        "* Названия мебели из ИКЕА\n",
        "* Мотивационные лозунги\n",
        "* etc.\n",
        "\n",
        "__Удачи!__"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HRiZOycSeopf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}